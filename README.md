# CS325_Proj1
Maximum subsequence sum analysis of various implementations.

There are three python scripts and a few helper shell scripts. 2 of the python 
scripts (problems.py and benchmark.py) are the entry points of the programs ie 
the main functions for the various ways to run the program. The third 
(algorithms.py) has the implementations of the 4 algorithms along with a couple
helper functions and a data structure to conveniently store the result data in.

The most important files for running the program are "problems.py" and
"benchmark.py". Run "problems.py --help" or "benchmark.py --help" for full
details on running the programs.

File inventory...

algorithms.py - The 4 important algorithms we are comparing implemented in 
   "enumerationAlgorithm", "betterEnumerationAlgorithm", 
   "divideAndConquerAlgorithm", and "linearAlgorithm"
   This script is not meant to be executed directly

problems.py - For running test problems against. There are a couple of examples
   of the input file in the package: MSS_Problems.txt and test-problems.txt
   The file can be run on the cli like so "problems.py someInputFile.txt" and
   and it takes various cli args to modify it's behaviour. By default it 
   prints to stdout but this can be overridden with a cli arg.

   The output file of MSS_Results.txt was generated by running..

      problems.py -f MSS_Results.txt MSS_Problems.txt

   Run...

      problems.py --help

   for a full list of cli options.

benchmark.py - For running benchmark times against the various algorithm
   implementations. This program generates several sets of random data
   and times the running of the algorithms agains the data. The amount of data
   to generate, the number of runs to average, and the algorithm to use are
   all controlled by cli args. A sample run would be something like so...
      
      benchmark.py -a divide --steps 9 --stepsize 100 --avg 10

   This would run the divide and conquer algorithm. This would runs of
   100,200...800,900 with each run done 10 times and the output time the
   average of 10 runs ie it would do 10 runs at 100 and output the average of
   all 10 runs runtimes.
   Run...

      benchmarks.py --help  

   ...for a full list of cli options.

MSS_Problems.txt - test problem file provided by prof

MSS_Results.txt - output file generated by running
   "problems.py -f MSS_Results.txt MSS_Problems.txt"
   Note - this output would be generated by the linear algorithm implementation
   by default but can be controlled with a cli arg. See "problems.py --help"

test-problems.txt - Some sample data problems used during development

test-solutions.txt - The "correct" answers to the problems in test-problems.txt
   Used to compare output of program for various algorithm implentations to 
   verify correctness.

dataruns.sh - Helper script to run the benchmark program for given data set 
   sizes for all implementations and output results to text files for easy 
   import into spreadsheets

package.sh - Helper script to zip up all the files for final submission
